---
# ============================================================================
# TUTORIAL: Vision-Language-Action (VLA) Models for Robotics
# ============================================================================

# REQUIRED METADATA (Replace ALL values below)
title: Vision-Language-Action (VLA) Models for Robotics
description: Learn how to integrate Vision-Language-Action models with robotics systems for multimodal AI control
keywords: [vla, vision-language-action, multimodal ai, robotics, openai, gpt, whisper, robotics ai]
sidebar_position: 1
learning_objectives:
  - Understand the concept of Vision-Language-Action (VLA) models in robotics
  - Apply multimodal AI models to interpret natural language commands for robots
  - Implement a system that translates voice commands to robotic actions
  - Demonstrate the integration of VLA models with ROS 2 and simulation environments

# Prerequisites: paths to required chapters (use "docs/path/to/chapter" format)
# Use empty array [] if no prerequisites
prerequisites:
  - docs/module-3-isaac/week-8-isaac-sim-expanded
  - docs/module-3-isaac/week-9-isaac-ros-vslam
  - docs/module-3-isaac/week-10-nav2-rl

# Estimated completion time in minutes (must be multiple of 5: 15, 30, 45, 60, etc.)
estimated_time: 120

# Content type (DO NOT CHANGE for this template)
content_type: tutorial

# Difficulty level: beginner | intermediate | advanced
difficulty: advanced

# OPTIONAL METADATA (Uncomment if needed)
# tags: [vla, multimodal, ai, robotics, gpt, whisper]  # For tag-based filtering
# draft: true  # Hide from production builds during authoring
# pagination_prev: docs/previous-chapter  # Override default previous link
# pagination_next: docs/next-chapter      # Override default next link
---

<LearningObjectives objectives={frontMatter.learning_objectives} />
<Prerequisites prereqs={frontMatter.prerequisites} estimatedTime={frontMatter.estimated_time} />

# {frontMatter.title}

## Introduction

In this tutorial, you'll learn how to integrate Vision-Language-Action (VLA) models with robotics systems for multimodal AI control. By the end, you'll have a system that interprets natural language commands and translates them into robotic actions.

**Why this matters:** Vision-Language-Action models represent the convergence of multiple AI domains, enabling robots to understand and execute complex natural language commands in real-world environments. This is essential for developing intuitive human-robot interaction.

## Prerequisites and Setup

Before you begin, ensure you have:

- **Software:** ROS 2 Humble, Isaac Sim, OpenAI API access
- **Hardware:** GPU with CUDA support (for local models) or internet access (for API-based models)
- **API Keys:** OpenAI API key for GPT and Whisper models
- **Knowledge:** Understanding of ROS 2, Isaac Sim, and basic AI concepts

### Verify Your Environment

```bash
# Check if ROS 2 is properly sourced
echo $ROS_DISTRO
# Expected output: humble

# Check if Python is available
python3 --version
# Expected output: Python 3.8 or higher
```

## Step 1: Set up the VLA Development Environment

Install the necessary packages for working with Vision-Language-Action models.

### Instructions

1. Create a new ROS 2 package for VLA integration:
   ```bash
   cd ~/ros2_ws/src
   ros2 pkg create --build-type ament_python vla_integration
   ```

2. Navigate to the package directory:
   ```bash
   cd ~/ros2_ws/src/vla_integration
   ```

3. Install Python dependencies:
   ```bash
   pip3 install openai speechrecognition pyttsx3 numpy opencv-python
   ```

4. Create the package structure:
   ```bash
   mkdir -p vla_integration/nodes
   touch vla_integration/__init__.py
   touch vla_integration/nodes/__init__.py
   ```

5. Update the package.xml file to include dependencies:
   ```xml
   <?xml version="1.0"?>
   <?xml-model href="http://download.ros.org/schema/package_format3.xsd" schematypens="http://www.w3.org/2001/XMLSchema"?>
   <package format="3">
     <name>vla_integration</name>
     <version>0.0.0</version>
     <description>Vision-Language-Action integration for robotics</description>
     <maintainer email="your.email@example.com">Your Name</maintainer>
     <license>Apache-2.0</license>

     <depend>rclpy</depend>
     <depend>std_msgs</depend>
     <depend>geometry_msgs</depend>
     <depend>sensor_msgs</depend>

     <test_depend>ament_copyright</test_depend>
     <test_depend>ament_flake8</test_depend>
     <test_depend>ament_pep257</test_depend>
     <test_depend>python3-pytest</test_depend>

     <export>
       <build_type>ament_python</build_type>
     </export>
   </package>
   ```

6. Update the setup.py file:
   ```python
   from setuptools import setup
   import os
   from glob import glob

   package_name = 'vla_integration'

   setup(
       name=package_name,
       version='0.0.0',
       packages=[package_name, f'{package_name}.nodes'],
       data_files=[
           ('share/ament_index/resource_index/packages',
            ['resource/' + package_name]),
           ('share/' + package_name, ['package.xml']),
       ],
       install_requires=['setuptools'],
       zip_safe=True,
       maintainer='Your Name',
       maintainer_email='your.email@example.com',
       description='Vision-Language-Action integration for robotics',
       license='Apache-2.0',
       tests_require=['pytest'],
       entry_points={
           'console_scripts': [
               'vla_node = vla_integration.nodes.vla_node:main',
           ],
       },
   )
   ```

**Expected result:** A new ROS 2 package is created with the necessary structure for VLA integration.

## Step 2: Implement Voice Recognition Component

Create a component to recognize voice commands using speech-to-text technology.

### Instructions

1. Create a voice recognition module:
   ```bash
   touch vla_integration/nodes/voice_recognizer.py
   ```

2. Edit the voice recognizer file:
   ```python
   import speech_recognition as sr
   import threading
   import queue


   class VoiceRecognizer:
       def __init__(self):
           self.recognizer = sr.Recognizer()
           self.microphone = sr.Microphone()
           
           # Adjust for ambient noise
           with self.microphone as source:
               self.recognizer.adjust_for_ambient_noise(source)
           
           self.result_queue = queue.Queue()
           self.listening = False
           self.listen_thread = None
           
       def start_listening(self):
           """Start listening for voice commands in a separate thread"""
           if not self.listening:
               self.listening = True
               self.listen_thread = threading.Thread(target=self._listen_continuously)
               self.listen_thread.start()
               
       def stop_listening(self):
           """Stop listening for voice commands"""
           self.listening = False
           if self.listen_thread:
               self.listen_thread.join()
               
       def _listen_continuously(self):
           """Continuously listen for voice commands"""
           with self.microphone as source:
               while self.listening:
                   try:
                       # Listen for audio with timeout
                       audio = self.recognizer.listen(source, timeout=1.0, phrase_time_limit=5)
                       
                       # Recognize speech using Google's free API
                       text = self.recognizer.recognize_google(audio)
                       self.result_queue.put(text)
                       
                   except sr.WaitTimeoutError:
                       # No speech detected, continue listening
                       continue
                   except sr.UnknownValueError:
                       # Speech not understood, continue listening
                       continue
                   except sr.RequestError as e:
                       # API request error, put error in queue
                       self.result_queue.put(f"Error: {str(e)}")
                       continue
                       
       def get_latest_command(self):
           """Get the latest recognized command, non-blocking"""
           try:
               # Get the latest command from the queue
               latest = None
               while not self.result_queue.empty():
                   latest = self.result_queue.get_nowait()
               return latest
           except queue.Empty:
               return None
   ```

3. Make the file executable:
   ```bash
   chmod +x vla_integration/nodes/voice_recognizer.py
   ```

**Expected result:** A voice recognition module is created that can continuously listen for voice commands.

## Step 3: Implement Language Processing Component

Create a component to process natural language commands using OpenAI's GPT model.

### Instructions

1. Create a language processing module:
   ```bash
   touch vla_integration/nodes/language_processor.py
   ```

2. Edit the language processor file:
   ```python
   import openai
   import os
   import json
   from typing import Dict, List, Optional


   class LanguageProcessor:
       def __init__(self, api_key: str = None):
           # Set the OpenAI API key
           if api_key:
               openai.api_key = api_key
           elif os.getenv("OPENAI_API_KEY"):
               openai.api_key = os.getenv("OPENAI_API_KEY")
           else:
               raise ValueError("OpenAI API key not provided")
               
           self.model = "gpt-4o-mini"  # Using a more affordable model for frequent use
           
       def process_command(self, command: str) -> Dict:
           """
           Process a natural language command and return structured robot actions.
           
           Args:
               command: Natural language command from user
               
           Returns:
               Dictionary containing robot actions to execute
           """
           # Define the system prompt to guide the AI's behavior
           system_prompt = """
           You are a robot command interpreter. Your job is to convert natural language 
           commands into structured robot actions. Respond only with a JSON object 
           containing the robot actions to perform.
           
           The possible actions are:
           - move_forward: Move the robot forward by a distance
           - move_backward: Move the robot backward by a distance
           - turn_left: Turn the robot left by an angle
           - turn_right: Turn the robot right by an angle
           - stop: Stop the robot
           - pick_up: Pick up an object
           - place_down: Place down an object
           - speak: Speak a message aloud
           
           Each action should have a value indicating magnitude/duration where applicable.
           For example: {"move_forward": 1.0} means move forward 1 meter.
           """
           
           try:
               response = openai.chat.completions.create(
                   model=self.model,
                   messages=[
                       {"role": "system", "content": system_prompt},
                       {"role": "user", "content": command}
                   ],
                   temperature=0.1,  # Low temperature for consistent responses
                   max_tokens=200,
                   response_format={"type": "json_object"}  # Request JSON response
               )
               
               # Extract the JSON response
               response_text = response.choices[0].message.content
               actions = json.loads(response_text)
               
               return actions
               
           except Exception as e:
               return {"error": f"Failed to process command: {str(e)}"}
               
       def validate_actions(self, actions: Dict) -> bool:
           """
           Validate that the actions are safe and appropriate for the robot.
           
           Args:
               actions: Dictionary of actions to validate
               
           Returns:
               True if actions are valid, False otherwise
           """
           # Define allowed actions
           allowed_actions = {
               "move_forward", "move_backward", "turn_left", "turn_right", 
               "stop", "pick_up", "place_down", "speak"
           }
           
           # Check if all actions are allowed
           for action in actions.keys():
               if action not in allowed_actions:
                   return False
                   
           # Additional validation could go here
           # For example, check if movement distances are reasonable
           if "move_forward" in actions:
               distance = actions["move_forward"]
               if not isinstance(distance, (int, float)) or distance <= 0 or distance > 10:
                   return False
                   
           return True
   ```

3. Make the file executable:
   ```bash
   chmod +x vla_integration/nodes/language_processor.py
   ```

**Expected result:** A language processing module is created that can convert natural language commands to structured robot actions.

## Step 4: Implement Action Execution Component

Create a component to execute robot actions based on processed commands.

### Instructions

1. Create an action execution module:
   ```bash
   touch vla_integration/nodes/action_executor.py
   ```

2. Edit the action executor file:
   ```python
   import rclpy
   from rclpy.node import Node
   from geometry_msgs.msg import Twist
   from std_msgs.msg import String
   import math
   import pyttsx3


   class ActionExecutor(Node):
       def __init__(self):
           super().__init__('action_executor')
           
           # Create publisher for robot velocity commands
           self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)
           
           # Create publisher for speaking commands
           self.speech_pub = self.create_publisher(String, '/speech', 10)
           
           # Timer for continuous action execution
           self.action_timer = self.create_timer(0.1, self.execute_action)
           
           # Initialize text-to-speech engine
           self.tts_engine = pyttsx3.init()
           self.tts_engine.setProperty('rate', 150)  # Speed of speech
           self.tts_engine.setProperty('volume', 0.9)  # Volume level
           
           # Current action to execute
           self.current_action = None
           self.action_remaining_time = 0.0
           self.action_start_time = self.get_clock().now()
           
       def execute_action(self):
           """Execute the current action"""
           if self.current_action is None:
               return
               
           # Calculate elapsed time since action started
           current_time = self.get_clock().now()
           elapsed = (current_time.nanoseconds - self.action_start_time.nanoseconds) / 1e9
           
           if self.action_remaining_time > 0:
               # Still executing the action
               self._publish_current_action()
               self.action_remaining_time -= 0.1  # Timer interval
           else:
               # Action completed, stop the robot
               self._stop_robot()
               self.current_action = None
               
       def _publish_current_action(self):
           """Publish the current action to the robot"""
           cmd_vel = Twist()
           
           if self.current_action == "move_forward":
               cmd_vel.linear.x = 0.5  # Move forward at 0.5 m/s
           elif self.current_action == "move_backward":
               cmd_vel.linear.x = -0.5  # Move backward at 0.5 m/s
           elif self.current_action == "turn_left":
               cmd_vel.angular.z = 0.5  # Turn left at 0.5 rad/s
           elif self.current_action == "turn_right":
               cmd_vel.angular.z = -0.5  # Turn right at 0.5 rad/s
           elif self.current_action == "stop":
               cmd_vel.linear.x = 0.0
               cmd_vel.angular.z = 0.0
               
           self.cmd_vel_pub.publish(cmd_vel)
           
       def _stop_robot(self):
           """Stop the robot"""
           cmd_vel = Twist()
           cmd_vel.linear.x = 0.0
           cmd_vel.angular.z = 0.0
           self.cmd_vel_pub.publish(cmd_vel)
           
       def execute_actions(self, actions: dict):
           """Execute a dictionary of actions"""
           for action, value in actions.items():
               if action == "move_forward":
                   self._execute_move_action(value, linear_x=0.5)
               elif action == "move_backward":
                   self._execute_move_action(value, linear_x=-0.5)
               elif action == "turn_left":
                   self._execute_turn_action(value, angular_z=0.5)
               elif action == "turn_right":
                   self._execute_turn_action(value, angular_z=-0.5)
               elif action == "stop":
                   self._stop_robot()
               elif action == "speak":
                   self._speak_message(value)
               elif action == "pick_up":
                   self.get_logger().info("Pick up action received - not implemented in simulation")
               elif action == "place_down":
                   self.get_logger().info("Place down action received - not implemented in simulation")
               elif action == "error":
                   self.get_logger().error(f"Error in command processing: {value}")
                   
       def _execute_move_action(self, distance, linear_x):
           """Execute a movement action"""
           # Calculate time needed to travel the distance (time = distance / speed)
           time_needed = abs(distance) / abs(linear_x)
           
           self.current_action = "move_forward" if linear_x > 0 else "move_backward"
           self.action_remaining_time = time_needed
           self.action_start_time = self.get_clock().now()
           
       def _execute_turn_action(self, angle, angular_z):
           """Execute a turning action"""
           # Calculate time needed to turn the angle (time = angle / angular_speed)
           time_needed = abs(angle) / abs(angular_z)
           
           self.current_action = "turn_left" if angular_z > 0 else "turn_right"
           self.action_remaining_time = time_needed
           self.action_start_time = self.get_clock().now()
           
       def _speak_message(self, message):
           """Speak a message aloud"""
           self.get_logger().info(f"Speaking: {message}")
           
           # Publish to speech topic
           speech_msg = String()
           speech_msg.data = message
           self.speech_pub.publish(speech_msg)
           
           # Also speak using local TTS engine
           self.tts_engine.say(message)
           self.tts_engine.runAndWait()
   ```

3. Make the file executable:
   ```bash
   chmod +x vla_integration/nodes/action_executor.py
   ```

**Expected result:** An action execution module is created that can execute robot actions based on processed commands.

## Step 5: Integrate Components into a VLA Node

Combine all components into a single ROS 2 node.

### Instructions

1. Create the main VLA node:
   ```bash
   touch vla_integration/nodes/vla_node.py
   ```

2. Edit the VLA node file:
   ```python
   import rclpy
   from rclpy.node import Node
   import os
   import time
   from .voice_recognizer import VoiceRecognizer
   from .language_processor import LanguageProcessor
   from .action_executor import ActionExecutor


   class VLANode(Node):
       def __init__(self):
           super().__init__('vla_node')
           
           # Initialize components
           self.voice_recognizer = VoiceRecognizer()
           self.language_processor = LanguageProcessor(os.getenv("OPENAI_API_KEY"))
           self.action_executor = ActionExecutor()
           
           # Start voice recognition
           self.voice_recognizer.start_listening()
           
           # Timer to periodically check for new commands
           self.command_timer = self.create_timer(1.0, self.process_commands)
           
           self.get_logger().info("VLA Node initialized and listening for commands...")
           
       def process_commands(self):
           """Process any new voice commands"""
           # Get the latest recognized command
           command = self.voice_recognizer.get_latest_command()
           
           if command and command.startswith("Error:") == False:
               self.get_logger().info(f"Recognized command: {command}")
               
               # Process the command with the language processor
               actions = self.language_processor.process_command(command)
               self.get_logger().info(f"Processed actions: {actions}")
               
               # Validate the actions
               if self.language_processor.validate_actions(actions):
                   # Execute the actions
                   self.action_executor.execute_actions(actions)
               else:
                   self.get_logger().error(f"Invalid actions: {actions}")
                   
           elif command and command.startswith("Error:"):
               self.get_logger().error(f"Voice recognition error: {command}")
               
       def destroy_node(self):
           """Cleanup when node is destroyed"""
           self.voice_recognizer.stop_listening()
           super().destroy_node()


   def main(args=None):
       rclpy.init(args=args)
       
       vla_node = VLANode()
       
       try:
           rclpy.spin(vla_node)
       except KeyboardInterrupt:
           pass
       finally:
           vla_node.destroy_node()
           rclpy.shutdown()


   if __name__ == '__main__':
       main()
   ```

3. Make the file executable:
   ```bash
   chmod +x vla_integration/nodes/vla_node.py
   ```

**Expected result:** A complete VLA node is created that integrates voice recognition, language processing, and action execution.

## Step 6: Build and Test the VLA System

Build the package and test the VLA system.

### Instructions

1. Build the workspace:
   ```bash
   cd ~/ros2_ws
   colcon build --packages-select vla_integration
   ```

2. Source the workspace:
   ```bash
   source install/setup.bash
   ```

3. Set your OpenAI API key as an environment variable:
   ```bash
   export OPENAI_API_KEY="your-openai-api-key-here"
   ```

4. Run the VLA node:
   ```bash
   ros2 run vla_integration vla_node
   ```

5. Speak voice commands to your computer's microphone (e.g., "Move forward 2 meters", "Turn left 90 degrees", "Stop").

6. Observe the robot's response in the terminal output.

**Expected result:** The VLA system recognizes voice commands, processes them with the language model, and executes appropriate actions.

## Step 7: Integrate with Simulation Environment

Connect the VLA system to a simulation environment like Isaac Sim or Gazebo.

### Instructions

1. Create a launch file to run the VLA system with a simulator:
   ```bash
   mkdir -p ~/ros2_ws/src/vla_integration/launch
   touch ~/ros2_ws/src/vla_integration/launch/vla_with_sim.launch.py
   ```

2. Edit the launch file:
   ```python
   import os
   from ament_index_python.packages import get_package_share_directory
   from launch import LaunchDescription
   from launch.actions import IncludeLaunchDescription, DeclareLaunchArgument, SetEnvironmentVariable
   from launch.launch_description_sources import PythonLaunchDescriptionSource
   from launch.substitutions import LaunchConfiguration
   from launch_ros.actions import Node


   def generate_launch_description():
       # Declare launch arguments
       openai_api_key_arg = DeclareLaunchArgument(
           'openai_api_key',
           default_value='',
           description='OpenAI API key for VLA processing'
       )
       
       # Set the OpenAI API key as an environment variable
       openai_api_key = LaunchConfiguration('openai_api_key')
       set_openai_key = SetEnvironmentVariable(
           name='OPENAI_API_KEY',
           value=openai_api_key
       )
       
       # Launch the VLA node
       vla_node = Node(
           package='vla_integration',
           executable='vla_node',
           name='vla_node',
           output='screen'
       )
       
       # Optionally, include a simulator launch file
       # Uncomment and adjust path as needed for your simulator
       # gazebo_launch = IncludeLaunchDescription(
       #     PythonLaunchDescriptionSource(
       #         os.path.join(get_package_share_directory('turtlebot3_gazebo'), 
       #                      'launch', 'empty_world.launch.py')
       #     )
       # )
       
       return LaunchDescription([
           openai_api_key_arg,
           set_openai_key,
           # gazebo_launch,  # Uncomment if using simulator
           vla_node
       ])
   ```

3. Build the package again:
   ```bash
   cd ~/ros2_ws
   colcon build --packages-select vla_integration
   source install/setup.bash
   ```

4. Launch the system with your API key:
   ```bash
   ros2 launch vla_integration vla_with_sim.launch.py openai_api_key:="your-openai-api-key-here"
   ```

**Expected result:** The VLA system runs integrated with a simulation environment, allowing voice commands to control a simulated robot.

## Key Takeaways

**What you learned:**
- How to implement a Vision-Language-Action system for robotics
- How to integrate voice recognition with AI language models
- How to convert natural language commands to robot actions
- How to connect the VLA system to simulation environments

## Next Steps

Now that you've mastered VLA integration, you're ready to:

1. **Try it yourself:** Enhance the system with computer vision capabilities to add the "Vision" component
2. **Go deeper:** Implement more sophisticated action planning and execution
3. **Apply it:** Use the VLA system in your capstone project for autonomous humanoid control

### Related Chapters

- [Conversational Robotics with GPT Models](../module-4-vla-humanoids/week-13-conversational-vla)
- [Capstone: Autonomous Humanoid Project](../../capstone/index)

## Practice Exercises

### Exercise 1: Add Computer Vision Component

**Challenge:** Enhance the VLA system to include computer vision capabilities. When a user says "Pick up the red ball," the system should use computer vision to locate the red ball and navigate to it.

**Hints:**
- Use OpenCV for image processing
- Subscribe to camera topics in ROS 2
- Implement color detection algorithms
- Combine navigation with object detection
- Consider using Isaac Sim's built-in computer vision capabilities

**Solution:** Available in `examples/vla-integration/computer-vision-enhancement.py`

---

## References

- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [ROS 2 Documentation](https://docs.ros.org/en/humble/)
- [Vision-Language-Action Models Research](https://arxiv.org/abs/2209.06588)
- [SpeechRecognition Library](https://pypi.org/project/SpeechRecognition/)